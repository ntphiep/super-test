{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python ETL script for TEST\") \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\", '8g')\\\n",
    "    .config(\"spark.sql.ansi.enabled \",True)\\\n",
    "    .config(\"spark.jars\", \"C:\\Drivers\\sqljdbc42.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "source_type = ''\n",
    "source_info = ''\n",
    "destination_db=''\n",
    "destination_schema=''\n",
    "destination_table = ''\n",
    "etl_type = ''\n",
    "query_string = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"TEST_DWH\"\n",
    "user = \"user\"\n",
    "password  = \"password\"\n",
    "\n",
    "\n",
    "query_string=\"SELECT a.*,CONCAT(ISNULL(b.status,'Pending'),b.status) status,null status_description ,null started_at,null completed_at FROM (SELECT *,getdate()  schedule_date FROM dbo.etl_metadata ) a LEFT JOIN [dbo].[etl_metadata_schedule] b ON a.id = b.id and  CAST(b.schedule_date AS date)= CAST(getdate() AS date) where ISNULL(b.status,'A') != 'completed'\"\n",
    "\n",
    "#Read ETL Meta Data\n",
    "etl_meta_data_staging = spark.read\\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:sqlserver://localhost:1433;databaseName={\"+database+\"};\") \\\n",
    "    .option(\"query\", query_string) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_meta_data_staging.filter(\"status == 'Pending'\").show()\n",
    "\n",
    "#THEN READ BASE META DATA AND  CREATE ONE ELSE DONT\n",
    "etl_meta_data_staging.filter(\"status == 'Pending'\").write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:sqlserver://localhost:1433;databaseName={\"+database+\"};\") \\\n",
    "        .option(\"dbtable\", \"dbo.etl_metadata_schedule\") \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .mode(\"append\")\\\n",
    "        .save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SQL SERVER CONNECTION TO MAINTAIN ERROR STATE-------------#\n",
    "conn = pyodbc.connect(\"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "                      \"Server=localhost,1433;\"\n",
    "                      \"Database=\"+database+\";\"\n",
    "                      \"UID=\"+user+\";\"\n",
    "                      \"PWD=\"+password+\";\")\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_etl_meta_data_staging  = etl_meta_data_staging.toPandas()\n",
    "df_etl_meta_data_staging = df_etl_meta_data_staging.sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for etl_id in df_etl_meta_data_staging['id']:\n",
    "   status = 'In Progress'\n",
    "   print(\"Starting for \"+ str(etl_id))\n",
    "   #---------------UPDATE In Progress Status---------------#\n",
    "   cursor.\\\n",
    "       execute('''UPDATE  [TEST_DWH].[dbo].[etl_metadata_schedule] \n",
    "               SET [status]=\\'''' \n",
    "               +status+ \"',[started_at]= CURRENT_TIMESTAMP where id= '\"+ str(etl_id)+\"';\")\n",
    "   conn.commit()\n",
    "\n",
    "   # load meta data into variables\n",
    "   source_type = df_etl_meta_data_staging['source_type'][df_etl_meta_data_staging['id']==etl_id].values[0]\n",
    "   source_info = df_etl_meta_data_staging['source_info'][df_etl_meta_data_staging['id']==etl_id].values[0]\n",
    "   destination_db = df_etl_meta_data_staging['destination_db'][df_etl_meta_data_staging['id']==etl_id].values[0]\n",
    "   destination_schema = df_etl_meta_data_staging['destination_schema'][df_etl_meta_data_staging['id']==etl_id].values[0]\n",
    "   destination_table = df_etl_meta_data_staging['destination_table'][df_etl_meta_data_staging['id']==etl_id].values[0]\n",
    "   etl_type = df_etl_meta_data_staging['etl_type'][df_etl_meta_data_staging['id']==etl_id].values[0]\n",
    "\n",
    "# initialize empty status for each run\n",
    "status = ''\n",
    "\n",
    "# Read  data from spurce try to read otherwise through exception\n",
    "   try: \n",
    "        print(\"Reading via \", source_info)\n",
    "        # Read CSV data \n",
    "        if source_type == 'CSV':\n",
    "           jdbcDF = spark.read\\\n",
    "                       .format(\"csv\") \\\n",
    "                       .option(\"header\", \"true\") \\\n",
    "                       .option(\"quote\", \"\\\"\") \\\n",
    "                       .option(\"escape\", \"\\\"\") \\\n",
    "                       .load(source_info) \n",
    "\n",
    "           status= 'read_successful'\n",
    "           jdbcDF.show()\n",
    "\n",
    "        elif source_type == 'sqlserver':\n",
    "            jdbcDF = spark.read\\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", \"jdbc:sqlserver://localhost:1433;databaseName={\"+database+\"};\") \\\n",
    "                .option(\"query\", source_info) \\\n",
    "                .option(\"user\", user) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "                .load() \n",
    "\n",
    "        #Try to Write Extracted data relevant to destination table\n",
    "        try:\n",
    "          jdbcDF.write \\\n",
    "                  .format(\"jdbc\") \\\n",
    "                  .option(\"url\", \"jdbc:sqlserver://localhost:1433;databaseName={\"+destination_db+\"};\") \\\n",
    "                  .option(\"dbtable\", destination_schema+\".\"+destination_table) \\\n",
    "                  .option(\"user\", user) \\\n",
    "                  .option(\"password\", password) \\\n",
    "                  .option(\"truncate\", \"true\") \\\n",
    "                  .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "                  .mode(\"overwrite\")\\\n",
    "                  .save()\n",
    "\n",
    "          status = 'completed'\n",
    "          print(\"Write Successful\")\n",
    "          #---------------UPDATE Success Status---------------#\n",
    "          cursor.\\\n",
    "              execute('''UPDATE  [TEST_DWH].[dbo].[etl_metadata_schedule] \n",
    "                      SET [status]=\\'''' \n",
    "                      +status+ \"',[completed_at]= CURRENT_TIMESTAMP where id= '\"+ str(etl_id)+\"';\")\n",
    "          conn.commit()\n",
    "          #---------------UPDATE Success Status---------------#\n",
    "\n",
    "        #except of Write Extracted data relevant to destination table\n",
    "        #---------------UPDATE Success Status---------------#\n",
    "        except Exception as e :\n",
    "              print('some error in writing')\n",
    "              status = 'error in writing to destination db, '+str(e)\n",
    "              #---------------UPDATE Error Status---------------#\n",
    "              cursor.\\\n",
    "                  execute('''UPDATE  [TEST_DWH].[dbo].[etl_metadata_schedule] \n",
    "                          SET [status]=\\'''' \n",
    "                          +status+ \"',[completed_at]= CURRENT_TIMESTAMP where id= '\"+ str(etl_id)+\"';\")\n",
    "              conn.commit()\n",
    "              #---------------UPDATE Error Status---------------#\n",
    "    #except of Read module data\n",
    "   except Exception as e :\n",
    "         print(\"some error in reading from source\")\n",
    "         status = 'error reading source , '+str(e)\n",
    "         print(status)\n",
    "         #---------------UPDATE Error Status---------------#\n",
    "         cursor.\\\n",
    "             execute('''UPDATE  [TEST_DWH].[dbo].[etl_metadata_schedule] \n",
    "                     SET [status]=\\'''' \n",
    "                     +status+ \"',[completed_at]= CURRENT_TIMESTAMP where id= '\"+ str(etl_id)+\"';\")\n",
    "         conn.commit()\n",
    "         #---------------UPDATE Error Status---------------#"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
